---
version: "2"
services:
  backend:
    build:
      context: ./customer-support-usecase-backend
    container_name: customer-support-usecase-backend
    image: customer-support-usecase-backend
    volumes:
      - /home/akarx/customer_support_usecase/volumes/vectordb:/vector_db
      - /models:/root/.cache/huggingface/hub
    ports:
      - 5002:8000
    depends_on:
      - tgi
      - database
    cpuset: "0-55,112-167"
    environment:
      - MONGO_URL=mongodb://database:27017/
      - INFERENCE_URL=http://tgi:80
      - VECTOR_DB_LOCATION=/vector_db/faiss_database
      - DATABASE_TYPE=test
      - CHUNK_SIZE=4000
      - CHUNK_OVERLAP=800
      - CHUNK_COUNT=10
  frontend:
    build:
      context: ./customer-support-usecase-frontend
    container_name: customer-support-usecase-frontend
    image: customer-support-usecase-frontend
    ports:
      - 5001:5173
    depends_on:
      - tgi
      - database
      - backend
    environment:
      - VITE_BACKEND_URL=http://localhost:5002
      - VITE_SESSION_ID=656f20b414828b118cef987b
      - VITE_SSE_URL=http://localhost:5003
      - VITE_MAX_NEW_TOKENS=300
  tgi:
    container_name: tgi-llama-2
    image: ghcr.io/huggingface/text-generation-inference:1.2
    command:
      [
        "--model-id",
        "mistralai/Mixtral-8x7B-Instruct-v0.1",
        "--max-input-length",
        "2048",
        "--max-total-tokens",
        "4096",
        "-v",
        "/home/akarx/cache/huggingface/hub:"
      ]
    volumes: [/home/akarx/.cache/huggingface/hub:/models]
    environment:
      - HUGGING_FACE_HUB_TOKEN=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
    cpuset: "0-55,112-167"
    shm_size: 30g
    ports:
      - "5003:80"
  database:
    image: mongo
    container_name: mongodb
    volumes: [/home/akarx/customer_support_usecase/volumes/mongodb:/data/db]
